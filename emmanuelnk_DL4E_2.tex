\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{graphics}
\usepackage{lmodern}
\usepackage{tikz}
\usepackage{microtype}
\usepackage[colorlinks = true,
linkcolor = blue,
urlcolor  = blue,
citecolor = magenta,
anchorcolor = blue]{hyperref}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{caption,subcaption}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
	
	\title{Deep Learning for Ecology report on bird recording at INTAKA island}
	
	\author{\IEEEauthorblockN{ Emmanuel \textsc{Nyandu Kagarabi}}
		\IEEEauthorblockA{\textit{African Institute for Mathematical Sciences} \\
			\textit{\textsc{Aims South Africa}}\\
			Cape Town, South Africa \\
			\texttt{emmanuelnk@aims.ac.za}}}
		\maketitle
	
\begin{abstract}
Theory is good, but practice is better. In this report, we present the quintessence of our assignment where we used the Convolutional Neural Networks (CNNs) to study bird sounds recorded at INTAKA island using  Raspberry Pi Zero ARU on February 3, 2024.		
\end{abstract}
	
	\begin{IEEEkeywords}
	Deep Learning, ecology, spectrogram, Raspberry Pi Zero 
	ARU, CNN.
	\end{IEEEkeywords}
%Progress in machine learning [177] has skyrocketed over recent years. Ma-
%chine learning has been applied to a vast number of domains such as product
%recommendations, social media analysis, search engines, face identification
%and emotion analysis. Machine learning is transforming academic research
%by enabling researchers to gain insights into problems which would take hu-
%mans a great amount of time\cite[p.1]{dufourq2019evolutionary}.	
	\section{Introduction}
The latest report from the Intergovernmental Panel on Climate Change (\href{https://www.ecologie.gouv.fr/comprendre-giec}{Groupe d’experts Intergouvernemental sur l’Evolution du Climat (GIEC)}) shows that the cause of global warming since 1850 is mainly man-made. As a result of our greenhouse gas emissions, we have succeeded in modifying our planet's climate, resulting in a total global surface temperature rise of $ 1.07^\circ\text{C} $. GIEC's experts are warning of the consequences of global warming: an increase in the frequency and intensity of extreme phenomena (tropical cyclones, heat waves, droughts, floods), rising sea levels, melting sea ice, glaciers and permafrost. This disruption has a direct impact on terrestrial and ocean ecosystems, threatening their functioning and the biodiversity that supports them, their functioning, as well as the biodiversity of which they are composed \cite[p.3]{jeantet2021identification}. According to the same report, a global warming of $ 1.5 ^\circ\text{C} $ compared to the average pre-modern global temperature,
6 \% of insects, 8 \% of plants and 4 \% of vertebrates would lose more than half their geographic range. Without the capacity to adapt rapidly to global warming, species could be doomed to extinction.  So
the study of behaviours is an important element in understanding how individuals
use their habitat, identify their capacity to adapt to climate change and identify the factors that threaten the equilibrium of ecosystems. Numerous studies have demonstrated the importance of studying behaviours
for the conservation of endangered species. 

 Nevertheless, observing behavior in the wild remains a difficult process, depending on environmental conditions, and the presence of an observer generally has an impact on the behavior of the species studied.   It is therefore important to develop methods for studying the behavior of species in their natural environment, while limiting the impact of this observation\cite{jeantet2021identification}. These 21st-century crises highlight the need, more than ever,
to study living things in order to better protect them. That's what we're trying to do with the Deep Learning for Ecology lecture, more especially the Bio-acoustics section, which is by definition  the exploration of animal vocalizations and natural soundscapes\cite{jeantet2020behavioural}, aspects developed in a such study's methodology can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies \cite[p.2]{jeantet2023improving}. Bioacoustics, has proven to be a valuable source of data for both a better understanding of animal behavior and for biodiversity monitoring. Over the past few years, deep learning has revolutionized several
	research fields such as bioinformatics  and medicine by enabling automated processing of large and
	complex datasets. Considered a branch of machine learning, deep
	learning refers to algorithms, commonly called deep neural networks,
	able to automatically detect very complex and highly discriminating
	patterns in data\cite{jeantet2023improving}.
	
	The succession of processing layers
	performing linear and non-linear transformations allows the neural
	networks to learn representations of data with multiple levels of
	abstraction. This ability makes deep learning
	particularly relevant for solving complex problems such as speech
	recognition, object detection, computer vision, and many other domains. Moreover, the most common
	approach in deep learning in bioacoustics is to convert the raw acoustic
	recordings into a succession of spectrograms or mel-spectrograms – a
	visual representation of the frequency spectrum of sound over time – and
	treat it as an image classification problem. These spec­trograms generally give a fixed image in time of the sounds and deprive them of their context\cite{jeantet2023improving}.
Having collected bird sounds at INTAKA island, we are using Convolutional Neural Networks in this work to discern the presence or absence of a bird sound in any given audio recording in our  dataset.

	\section{Materials and methods}

	
\subsection{Data collection}
It all began on Friday, February 02, 2024, when we were taught to assemble the Raspberry Pi Zero ARU kit in the lecture hall. The Raspberry Pi Zero is an incredibly small and affordable single-board computer developed by the Raspberry Pi Foundation. It's designed to be extremely compact, while offering sufficient processing power for a variety of applications (For example automatic recording in this report). Here, below (Fig. \ref{fig:screenshot-from-2024-02-17-16-37-58}), is the complete kit with its various parts.
 
\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Screenshot from 2024-02-17 16-37-58"}
	\caption{Raspberry Pi Zero 
		ARU 
	}
	\label{fig:screenshot-from-2024-02-17-16-37-58}
\end{figure}


The next day; Saturday 03/02/2024 was the big day when we left AIMS early for INTAKA island (Fig.\ref{fig:screenshot-from-2024-02-15-23-22-35}) to collect data. We left AIMS in the morning (at 7am) and arrived at INTAKA at 8am. Once there, in groups of two, each student with a complete recording kit, the good work began around 8:30 am and we were asked to disconnect the power bank every 5 minutes and reconnect it again. This was to optimize the recording because, without it, 

\begin{figure}[!h]
	\centering
\includegraphics[width=0.8\linewidth]{Enviro-Centre-09.2010.-281-1024x658}
\caption{\href{https://intaka.co.za/}{Intaka Island Nature Reserve, Century City, Cape Town}}
\label{fig:screenshot-from-2024-02-15-23-22-35}
\end{figure}
some people might assume that they were recording when the kit was probably not plugged in properly. At the same time, at the start of each recording, the geographical coordinates of the recording location were determined using the telephone's GPS. Around 11:45 am, it's time to go back at AIMS. Once back, the annotation work had begun. Samples were collected from each audio file. Conventionally, a sample in which the sound of a bird was present had to be 'annotated' with the digit \textbf{1}, and with \textbf{0} in the opposite case. We illustrate this with a screenshot (Fig. \ref{fig:screenshot-from-2024-02-17-16-14-06}) of the Sonic Visualiser window where the annotations were made.
\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Screenshot from 2024-02-17 16-14-06"}
	\caption{Sonic Visualiser}
	\label{fig:screenshot-from-2024-02-17-16-14-06}
\end{figure}

\subsection{Data pre-processing}
I annotated all my audio files and create a folder named after my ID : 08, within which I should create two sub-folders: 
\textbf{audios} for all my recordings (the\texttt{ .wav} files) and
\textbf{annotations} for the corresponding annotation files (the \texttt{.svl} files). Each of my comrades followed the same procedure, and together we compiled the $ AIMS\_BIRD\_DATASET $ folder, which was studied and corrected perfectly by a number of comrades, whom I congratulate in passing. At the end of the process, we all used a uniform database.
The desired folder structure is illustrated in the following class template (Fig. \ref{fig:screenshot-from-2024-02-18-00-39-27}).
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.4\linewidth]{"Screenshot from 2024-02-18 00-57-43"}
	\caption{\href{https://drive.google.com/drive/folders/1cG94j1OHZPMCgZ6wUc-2mbCuXkd8INQ7?usp=drive_link}{AIMS\_BIRD\_DATASET}}
	\label{fig:screenshot-from-2024-02-18-00-39-27}
\end{figure}



%Frequency masking is a phenomenon in digital signal processing in which a strong signal at one frequency covers or hides a weaker signal at a neighbouring frequency. Fundamentally, the stronger signal makes it difficult or impossible to identify or process the weaker signal, perhaps leading to processing mistakes or inaccuracies.


% which  involved randomly
%selecting a time point within the segment, and shifting the start of the
%segment to that time point, wrapping back on itself so that it remained 3s.
% Time masking involved applying a mask, t$\in$
%U(0, 100), to a spectrogram for which the start of the mask is randomly
%selected as $ t0\in [0, L-t] $, where L is the length of the spectrogram. Thus,
%$ [t0 , t0+t ) $ is masked out and replaced with a value of zero.
\subsection{Training and testing the models}

In my work, each recording was downsampled to 22,050 Hz and converted into a
mel-scale spectrogram to be used as an input image for Convolu­tional Neural Network (CNN). I performed a Hann analysis window
size of 46 ms (1,024 samples) with a hop size of 12 ms (256 samples) and
128 mel frequency bins. I restricted the
frequency range of the spectrograms between 150 Hz and 15 kHz as
most bird vocalizations occur between 250 Hz and 8.3 kHz. 
Using our manual annotations, the songs
were extracted from the spectrograms and divided into equal 3s seg­ments using a sliding window with an overlap of 1s. Each spectrogram
was thus an image of size 128 × 259.
I applied \href{https://www.soundgym.co/blog/item?id=four-ways-to-fix-frequency-masking-in-your-mix}{frequency masking} as a data
augmentation.  You can find easily the details of my model used into my jupiter notebook. The models were imple­mented in Python 3 using the TensorFlow and Keras libraries, and the audio processing and spectrogram construction were performed using Librosa.

	\section{Conclusion}
This intense work enabled us to combine theory and practice in the Deep Learning for Ecology course. With this work, we succeeded in implementing an algorithm for detecting the sound of a bird. We didn't use all the data available in our database, as RAM memory was running low and we had no option but to reduce the size of the data. For more information, please refer to our \texttt{.ipynb} file.

	
%	\begin{table}[htbp]
%		\caption{Table Type Styles}
%		\begin{center}
%			\begin{tabular}{|c|c|c|c|}
%				\hline
%				\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%				\cline{2-4} 
%				\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%				\hline
%				copy& More table copy$^{\mathrm{a}}$& &  \\
%				\hline
%				\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%			\end{tabular}
%			\label{tab1}
%		\end{center}
%	\end{table}
	


\section*{Data availability}
All code for training and testing the neural networks is available at my own Github repository  \url{https://github.com/emmanuelnyandukagarabi?tab=projects}.
\section*{Acknowledgments}
I would like to warmly thank our supervisors Dr Emmanuel Dufourq, Laurene and Matthew for introducing us to this very interesting field. At the same time, I'd like to thank all the AIMS colleagues with whom we share this historically exciting moment in our scientific career.
	%\section*{References}

\bibliographystyle{plain}
\bibliography{Emma_references}	
\end{document}